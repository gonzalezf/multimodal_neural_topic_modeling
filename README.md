
# Multimodal Neural Topic Models

This repository contains the code for our paper **Neural Multimodal Topic Modeling: A Comprehensive Evaluation
** presented at LREC-COLING 2024.

## Abstract

Neural topic models can successfully find coherent and diverse topics in textual data. However, they are limited in dealing with multimodal datasets (e.g., images and text). This paper presents the first systematic and comprehensive evaluation of multimodal topic modeling of documents containing both text and images. In the process, we propose two novel topic modeling solutions and two novel evaluation metrics. Overall, our evaluation on an unprecedented rich and diverse collection of datasets indicates that both of our models generate coherent and diverse topics. Nevertheless, the extent to which one method outperforms the other depends on the metrics and dataset combinations, which suggests further exploration of hybrid solutions in the future. Notably, our succinct human evaluation aligns with the outcomes determined by our proposed metrics. This alignment not only reinforces the credibility of our metrics but also highlights the potential for their application in guiding future multimodal topic modeling endeavors.

## Proposed Models

Our proposed models are based on the following frameworks:

- **Multimodal-ZeroShotTM:** We propose Multimodal-ZeroShotTM, a novel multimodal topic modeling algorithm based on ZeroShotTM (Bianchi et al., 2021b)
- **Multimodal-Contrast:**  We propose Multimodal-Contrast, an adaptation specifically derived from M3L-Contrast (Zosa and Pivovarova, 2022a). While M3L-Contrast is a neural topic modeling technique tailed for analyzing datasets that are both multilingual and multimodal, Multimodal-Contrast shifts the focus to solely multimodal data. 

## Code Availability

The code for our models and evaluation metrics will be released soon. Stay tuned for updates and the official release.

## How to Use This Repository

Instructions on how to use the code and replicate our results will be provided upon code release. This will include:

- Installation instructions
- Data preparation guidelines
- Running the models
- Evaluating the results

## Citation

Please cite our paper if you use our models or findings in your research. The citation information will be provided upon publication.

## Contact

For any queries or further discussion regarding our work, feel free to contact us. 

Felipe Gonz√°lez-Pizarro
felipeandresgonzalezpizarro[at] gmail [dot] com



**Thank you for your interest in our work on multimodal topic modeling!**

